---
title: "Training a Tabnet model from missing-values dataset"
author: "Christophe Regouby"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{pretrain from dataset with missing-values}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

# Motivation

Real-life training dataset usually contains missing data. The vast majority of deep-learning networks do not handle missing data and thus either stop or crash when values are missing in the predictors.

But Tabnet use a masking mechanism that can be adapted to cover the missing data in the training set.

# Missing-data dataset creation

## Ames

The `ames` dataset from the `modeldata` packages contains a lot of numerical value 0 that the human analysis clearly translate into missing data, like pool size of 0 square meters, basement surface of 0 square meters, ...A lot of them can be detected visually by inspecting the distribution of the values like for the `Masonry veneer area` predictor :
```{r}
suppressPackageStartupMessages(library(tidymodels))
library(tabnet)
data("ames", package = "modeldata")
qplot(ames$Mas_Vnr_Area)
```
![ames variable Mas_Vnr_Area histogram showing high occurence of value zero](ames_mas_vnr_hist.png)

## Ames with missing data

Let's dig up those missing data :

A quick and dirty way to achieve it is to `na_if()` zeros on surface and area columns. 

```{r}
col_with_zero_as_na <- ames %>% 
  select(where(is.numeric)) %>% 
  select(matches("_SF|Area|Misc_Val|[Pp]orch$")) %>% 
  summarise_each(min) %>% 
  select_if(~.x==0) %>% 
  names()
ames_missing <- ames %>% mutate_at(col_with_zero_as_na, na_if, 0) %>% 
  mutate_at("Alley", na_if, "No_Alley_Access") %>% 
  mutate_at("Pool_QC", na_if, "No_Pool") %>% 
  mutate_at("Fence", na_if, "No_Fence") %>% 
  mutate_at("Misc_Feature", na_if, "None") %>% 
  mutate_at(c("Garage_Cond", "Garage_Finish", "Garage_Type"), na_if, "No_Garage") %>% 
  mutate_at(c("Bsmt_Cond", "Bsmt_Exposure", "BsmtFin_Type_1", "BsmtFin_Type_2"), na_if, "No_Basement")
visdat::vis_miss(ames_missing)
```
![ames missing values visualisation showing few variables with more than 90% missingness with a global 16% missing](vis_miss_ames.png)

We can see here that variable are not missing at random, and thus we can expect the model to capture the missingness relation during the pretraining phase.

Note: A better way to achieve proper missing value to be explicit NAs would be to turn numerical value to NA when the corresponding description columns refer to `none` or to zero occurence of the equipment. But this is beyond the scope of this vignette.

# Model pretraining

Let's pretrain one model for each of those dataset, and analyse variable importance that emerge after the unsupervised representation learning step:   

## Variable importance with raw `ames` dataset

```{r}
ames_rec <- recipe(Sale_Price ~ ., data=ames) %>% 
  step_normalize(all_numeric())
ames_fit <- tabnet_pretrain(ames_rec, data=ames,  epoch=50, 
                            valid_split = 0.2, verbose=TRUE, batch=2930)
autoplot(ames_fit)
vip::vip(ames_fit)
```
```
[Epoch 001] Loss: 197.987808 Valid loss: 2521389.750000
[Epoch 002] Loss: 90.309837 Valid loss: 1317253.625000
[Epoch 003] Loss: 55.909924 Valid loss: 731009.875000
[Epoch 004] Loss: 40.682549 Valid loss: 592438.062500
[Epoch 005] Loss: 34.729988 Valid loss: 340338.500000
[Epoch 006] Loss: 27.579042 Valid loss: 287374.906250
[Epoch 007] Loss: 23.080545 Valid loss: 277046.562500
[Epoch 008] Loss: 20.220177 Valid loss: 256409.828125
...
[Epoch 045] Loss: 2.362013 Valid loss: 63409.878906
[Epoch 046] Loss: 2.217737 Valid loss: 60885.753906
[Epoch 047] Loss: 2.113867 Valid loss: 61390.011719
[Epoch 048] Loss: 2.358693 Valid loss: 58433.234375
[Epoch 049] Loss: 2.108813 Valid loss: 56608.714844
[Epoch 050] Loss: 2.016043 Valid loss: 57533.851562
```

![ames_fit model training diagnostic plot](ames_pretrain.png)
![ames_fit model variable importance plot](ames_pretrain_vip.png)

Training loss evolution seems correct and we get `Alley` and `Pool_QC` variables in the top ten important variables according to the pretrained model. Those two variables have been screened as having a very important missing rate.

## Variable importance with `ames_missing` dataset

Let's pretrain a new model with the same hyperparameter, but now using the `ames_missing` dataset.  
In order to compensate the 16% missingness already present in the `ames_missing` dataset, we adjust the `pretraining_ratio` parameter to `0.5 - 0.16 = 0.34`

```{r}
ames_missing_rec <- recipe(Sale_Price ~ ., data=ames_missing) %>% 
  step_normalize(all_numeric())
ames_missing_fit <- tabnet_pretrain(ames_missing_rec, data=ames_missing, epoch=50, 
                                    valid_split = 0.2, verbose=TRUE, batch=2930, 
                                    pretraining_ratio=0.34)
autoplot(ames_missing_fit)
vip::vip(ames_missing_fit)
```
```
[Epoch 001] Loss: 41.925888 Valid loss: 11.546161
[Epoch 002] Loss: 32.214478 Valid loss: 10.309987
[Epoch 003] Loss: 24.262951 Valid loss: 9.479233
[Epoch 004] Loss: 19.042051 Valid loss: 8.434237
[Epoch 005] Loss: 15.512108 Valid loss: 7.437159
[Epoch 006] Loss: 12.279278 Valid loss: 6.486411
[Epoch 007] Loss: 10.196947 Valid loss: 5.815371
[Epoch 008] Loss: 9.259965 Valid loss: 5.111839
...
[Epoch 045] Loss: 6.641933 Valid loss: 4.190426
[Epoch 046] Loss: 6.781425 Valid loss: 4.186747
[Epoch 047] Loss: 6.599716 Valid loss: 4.178854
[Epoch 048] Loss: 6.766137 Valid loss: 4.275692
[Epoch 049] Loss: 7.051480 Valid loss: 4.244986
[Epoch 050] Loss: 6.723494 Valid loss: 4.215664

```
![ames_missing_fit model training diagnostic plot](ames_missing_pretrain.png)
![ames_missing_fit model variable importance plot](ames_missing_pretrain_vip.png)

The model is slightly overfitting here, but we can see here no variables with high missingness is present in the top 10 important variables. This seems to be a good sign of the model having captured proper interactions between variables.

